# Веб-краулер для анализа сайтов

Этот инструмент позволяет сканировать веб-сайты, собирая информацию о внутренних и внешних ссылках, поддоменах, файлах и битых ссылках.

## Функционал

- Рекурсивное сканирование сайта (до 300 страниц по умолчанию)
- Обнаружение и классификация ссылок:
  - Внутренние ссылки
  - Внешние ссылки
  - Ссылки на файлы (PDF, DOC, DOCX)
- Выявление поддоменов
- Проверка работоспособности ссылок
- Статистика по сканированию

## Требования

- Python 3.x
- Установленные библиотеки:
  - requests
  - beautifulsoup4
  - tldextract

Установите зависимости командой:
```
pip install requests beautifulsoup4 tldextract
```

## Использование

1. Запустите скрипт:
   ```
   python crawler.py
   ```
2. Введите URL сайта для сканирования (например: `spbu.ru` или `https://spbu.ru`)
3. Дождитесь завершения сканирования
4. Просмотрите результаты в терминале

## Ограничения

- Максимальное количество сканируемых страниц: 300 (можно изменить в коде)
- Поддерживаются только HTTP/HTTPS ссылки
- Может быть заблокирован анти-краулерами сайта

## Выходные данные

После завершения сканирования программа выводит:
- Количество посещенных страниц
- Количество внутренних и внешних ссылок
- Список поддоменов
- Количество битых ссылок
- Ссылки на найденные документы
- Время выполнения
